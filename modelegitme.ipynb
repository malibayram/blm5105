{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-09 19:20:36.437024: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, GRU, Embedding, CuDNNGRU\n",
    "from tensorflow.python.keras.optimizers import adam_v2\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph_loss(history, metric, transparent=False):\n",
    "    plt.plot(history.history[metric])\n",
    "    plt.plot(history.history[\"val_\" + metric], \"\")\n",
    "    plt.xlabel(\"Devirler\")\n",
    "    plt.ylabel(\"Kayıp\")\n",
    "    plt.legend([\"Kayıp\", \"Devir Test Kaybı\"])\n",
    "    plt.savefig(\n",
    "        \"eticaret\" + metric + \".png\",\n",
    "        dpi=(250),\n",
    "        bbox_inches=\"tight\",\n",
    "        transparent=transparent,\n",
    "    )\n",
    "\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph_accuracy(history, metric, transparent=False):\n",
    "    plt.plot(history.history[metric])\n",
    "    plt.plot(history.history[\"val_\" + metric], \"\")\n",
    "    plt.xlabel(\"Devirler\")\n",
    "    plt.ylabel(\"Doğruluk\")\n",
    "    plt.legend([\"Doğruluk\", \"Devir Test Doğruluğu\"])\n",
    "    plt.savefig(\n",
    "        \"eticaret\" + metric + \".png\",\n",
    "        dpi=(250),\n",
    "        bbox_inches=\"tight\",\n",
    "        transparent=transparent,\n",
    "    )\n",
    "\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "yorumlar = pd.read_csv(\"yorumlar.csv\")\n",
    "target = yorumlar[\"Puan\"].values.tolist()\n",
    "data = yorumlar[\"Yorum\"].values.tolist()\n",
    "\n",
    "cutoff = int(len(data) * 0.90)\n",
    "x_train, x_test = data[:cutoff], data[cutoff:]\n",
    "y_train, y_test = target[:cutoff], target[cutoff:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./third_tokenizer.json\") as json_dosyasi:\n",
    "    json_tokenizer = json.load(json_dosyasi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19850"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_tokenizer['larla']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tf_tokenizer.json') as f:\n",
    "    json_string = json.load(f)\n",
    "\n",
    "tf_tokenizer = tf.keras.preprocessing.text.tokenizer_from_json(json_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "177704"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_tokenizer.word_index['tarla']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenlestir(yorumListesi):\n",
    "    y_yorumlar = []\n",
    "    for yorum in yorumListesi:\n",
    "        y_yorum = []\n",
    "        for kelime in str(yorum).lower().split():\n",
    "            if len(y_yorum) < 50 and kelime in json_tokenizer:\n",
    "                y_yorum.append(json_tokenizer[kelime])\n",
    "\n",
    "        if len(y_yorum) < 50:\n",
    "            sifirlar = list(np.zeros(50 - len(y_yorum), dtype=int))\n",
    "            y_yorum = sifirlar + y_yorum\n",
    "\n",
    "        y_yorumlar.append(y_yorum)\n",
    "    return np.array(y_yorumlar, dtype=np.dtype(np.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "egitim_kume = tokenlestir(x_train)\n",
    "test_kume = tokenlestir(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_layer (Embedding)  (None, 50, 50)            1000050   \n",
      "_________________________________________________________________\n",
      "gru_12 (GRU)                 (None, 50, 16)            3216      \n",
      "_________________________________________________________________\n",
      "gru_13 (GRU)                 (None, 50, 8)             600       \n",
      "_________________________________________________________________\n",
      "gru_14 (GRU)                 (None, 4)                 156       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 1,004,027\n",
      "Trainable params: 1,004,027\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(\n",
    "    Embedding(input_dim=20001, output_dim=50, input_length=50, name=\"embedding_layer\")\n",
    ")\n",
    "\n",
    "model.add(GRU(units=16, return_sequences=True, reset_after=False))\n",
    "model.add(GRU(units=8, return_sequences=True, reset_after=False))\n",
    "model.add(GRU(units=4, return_sequences=False, reset_after=False))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "opt = adam_v2.Adam(learning_rate=1e-3, clipnorm=1.0, clipvalue=0.5)\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.5299 - accuracy: 0.7679WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 30 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 30 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140/140 [==============================] - 18s 88ms/step - loss: 0.5299 - accuracy: 0.7679 - val_loss: 0.3937 - val_accuracy: 0.8610\n",
      "Epoch 2/25\n",
      "140/140 [==============================] - 11s 81ms/step - loss: 0.3448 - accuracy: 0.8839\n",
      "Epoch 3/25\n",
      "140/140 [==============================] - 11s 81ms/step - loss: 0.2956 - accuracy: 0.9037\n",
      "Epoch 4/25\n",
      "140/140 [==============================] - 11s 79ms/step - loss: 0.2631 - accuracy: 0.9177\n",
      "Epoch 5/25\n",
      "140/140 [==============================] - 11s 78ms/step - loss: 0.2419 - accuracy: 0.9258\n",
      "Epoch 6/25\n",
      "140/140 [==============================] - 11s 79ms/step - loss: 0.2255 - accuracy: 0.9330\n",
      "Epoch 7/25\n",
      "140/140 [==============================] - 11s 78ms/step - loss: 0.2124 - accuracy: 0.9373\n",
      "Epoch 8/25\n",
      "140/140 [==============================] - 11s 78ms/step - loss: 0.1988 - accuracy: 0.9431\n",
      "Epoch 9/25\n",
      "140/140 [==============================] - 11s 80ms/step - loss: 0.1891 - accuracy: 0.9469\n",
      "Epoch 10/25\n",
      "140/140 [==============================] - 11s 80ms/step - loss: 0.1781 - accuracy: 0.9514\n",
      "Epoch 11/25\n",
      "140/140 [==============================] - 11s 80ms/step - loss: 0.1695 - accuracy: 0.9547\n",
      "Epoch 12/25\n",
      "140/140 [==============================] - 10s 74ms/step - loss: 0.1636 - accuracy: 0.9567\n",
      "Epoch 13/25\n",
      "140/140 [==============================] - 12s 85ms/step - loss: 0.1573 - accuracy: 0.9592\n",
      "Epoch 14/25\n",
      "140/140 [==============================] - 11s 78ms/step - loss: 0.1526 - accuracy: 0.9609\n",
      "Epoch 15/25\n",
      "140/140 [==============================] - 11s 78ms/step - loss: 0.1494 - accuracy: 0.9617\n",
      "Epoch 16/25\n",
      "140/140 [==============================] - 11s 80ms/step - loss: 0.1448 - accuracy: 0.9634\n",
      "Epoch 17/25\n",
      "140/140 [==============================] - 11s 79ms/step - loss: 0.1415 - accuracy: 0.9644\n",
      "Epoch 18/25\n",
      "140/140 [==============================] - 11s 80ms/step - loss: 0.1381 - accuracy: 0.9654\n",
      "Epoch 19/25\n",
      "140/140 [==============================] - 12s 89ms/step - loss: 0.1374 - accuracy: 0.9658\n",
      "Epoch 20/25\n",
      "140/140 [==============================] - 11s 78ms/step - loss: 0.1358 - accuracy: 0.9661\n",
      "Epoch 21/25\n",
      "140/140 [==============================] - 12s 88ms/step - loss: 0.1310 - accuracy: 0.9681\n",
      "Epoch 22/25\n",
      "140/140 [==============================] - 13s 95ms/step - loss: 0.1288 - accuracy: 0.9687\n",
      "Epoch 23/25\n",
      "140/140 [==============================] - 12s 84ms/step - loss: 0.1279 - accuracy: 0.9692\n",
      "Epoch 24/25\n",
      "140/140 [==============================] - 12s 88ms/step - loss: 0.1250 - accuracy: 0.9702\n",
      "Epoch 25/25\n",
      "140/140 [==============================] - 13s 95ms/step - loss: 0.1229 - accuracy: 0.9709\n"
     ]
    }
   ],
   "source": [
    "n_y_train = np.array(y_train)\n",
    "n_y_test = np.array(y_test)\n",
    "\n",
    "history = model.fit(\n",
    "    egitim_kume,\n",
    "    n_y_train,\n",
    "    epochs=25,\n",
    "    batch_size=512,\n",
    "    validation_data=(test_kume, n_y_test),\n",
    "    validation_steps=30,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "249/249 [==============================] - 3s 12ms/step - loss: 0.4711 - accuracy: 0.8697\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_kume, n_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.47108957171440125\n",
      "Test Accuracy: 0.8696581125259399\n",
      "[[0.9590216 ]\n",
      " [0.95821506]\n",
      " [0.01047284]\n",
      " [0.95859253]\n",
      " [0.01047935]\n",
      " [0.220978  ]\n",
      " [0.95817643]\n",
      " [0.01048765]\n",
      " [0.9576007 ]\n",
      " [0.18795137]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Loss: {}\".format(test_loss))\n",
    "print(\"Test Accuracy: {}\".format(test_acc))\n",
    "\n",
    "text1 = \"bu ürün çok iyi herkese tavsiye ederim\"\n",
    "text2 = \"kargo çok hızlı aynı gün elime geçti\"\n",
    "text3 = \"büyük bir hayal kırıklığı yaşadım bu ürün bu markaya yakışmamış\"\n",
    "text4 = \"mükemmel\"\n",
    "text5 = \"tasarımı harika ancak kargo çok geç geldi ve ürün açılmıştı tavsiye etmem\"\n",
    "text6 = \"hiç resimde gösterildiği gibi değil\"\n",
    "text7 = \"kötü yorumlar gözümü korkutmuştu ancak hiçbir sorun yaşamadım teşekkürler\"\n",
    "text8 = \"hiç bu kadar kötü bir satıcıya denk gelmemiştim ürünü geri iade ediyorum\"\n",
    "text9 = \"tam bir fiyat performans ürünü\"\n",
    "text10 = \"beklediğim gibi çıkmadı\"\n",
    "yazilar = [text1, text2, text3, text4, text5, text6, text7, text8, text9, text10]\n",
    "\n",
    "deneme_kume = tokenlestir(yazilar)\n",
    "sonuc = model.predict(deneme_kume)\n",
    "print(sonuc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graph_accuracy(history, \"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graph_loss(history, \"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/my_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/my_model/assets\n"
     ]
    }
   ],
   "source": [
    "# Save the entire model as a SavedModel.\n",
    "!mkdir -p saved_model\n",
    "model.save('saved_model/my_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_layer (Embedding)  (None, 50, 50)           1000050   \n",
      "                                                                 \n",
      " gru_12 (GRU)                (None, 50, 16)            3216      \n",
      "                                                                 \n",
      " gru_13 (GRU)                (None, 50, 8)             600       \n",
      "                                                                 \n",
      " gru_14 (GRU)                (None, 4)                 156       \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 5         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,004,027\n",
      "Trainable params: 1,004,027\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "new_model = tf.keras.models.load_model('saved_model/my_model')\n",
    "\n",
    "# Check its architecture\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "y_pred = new_model.predict(test_kume)\n",
    "y_pred = (y_pred > 0.5)\n",
    "\n",
    "cm = confusion_matrix(n_y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 36ms/step\n",
      "[[0.01070459]\n",
      " [0.01220471]\n",
      " [0.95957804]\n",
      " [0.95780295]\n",
      " [0.95676   ]\n",
      " [0.8082179 ]\n",
      " [0.95817643]\n",
      " [0.01048765]\n",
      " [0.9576007 ]\n",
      " [0.18795137]]\n"
     ]
    }
   ],
   "source": [
    "text1 = \"ürün kaliteli ama kargo için yanı şeyi söyleyemem\"\n",
    "text2 = \"kargo zamanında elime ulaştı ancak içi fiyasko\"\n",
    "text3 = \"her şey istediğim gibi geldi, zaten fiyatı da öyleydi\"\n",
    "text4 = \"aldığım 10 ürünün 8 tanesi istediğim gibiydi\"\n",
    "text5 = \"kargo tam istediğim zamanında getirdi\"\n",
    "text6 = \"en büyük kargo benim kargom\"\n",
    "text7 = \"kötü yorumlar gözümü korkutmuştu ancak hiçbir sorun yaşamadım teşekkürler\"\n",
    "text8 = \"hiç bu kadar kötü bir satıcıya denk gelmemiştim ürünü geri iade ediyorum\"\n",
    "text9 = \"tam bir fiyat performans ürünü\"\n",
    "text10 = \"beklediğim gibi çıkmadı\"\n",
    "yazilar = [text1, text2, text3, text4, text5, text6, text7, text8, text9, text10]\n",
    "\n",
    "deneme_kume = tokenlestir(yazilar)\n",
    "sonuc = new_model.predict(deneme_kume)\n",
    "print(sonuc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
