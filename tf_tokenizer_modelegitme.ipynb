{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-08 22:36:40.405041: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, GRU, Embedding, CuDNNGRU\n",
    "from tensorflow.python.keras.optimizers import adam_v2\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph_loss(history, metric, transparent=False):\n",
    "    plt.plot(history.history[metric])\n",
    "    plt.plot(history.history[\"val_\" + metric], \"\")\n",
    "    plt.xlabel(\"Devirler\")\n",
    "    plt.ylabel(\"Kayıp\")\n",
    "    plt.legend([\"Kayıp\", \"Devir Test Kaybı\"])\n",
    "    plt.savefig(\n",
    "        \"eticaret\" + metric + \".png\",\n",
    "        dpi=(250),\n",
    "        bbox_inches=\"tight\",\n",
    "        transparent=transparent,\n",
    "    )\n",
    "\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph_accuracy(history, metric, transparent=False):\n",
    "    plt.plot(history.history[metric])\n",
    "    plt.plot(history.history[\"val_\" + metric], \"\")\n",
    "    plt.xlabel(\"Devirler\")\n",
    "    plt.ylabel(\"Doğruluk\")\n",
    "    plt.legend([\"Doğruluk\", \"Devir Test Doğruluğu\"])\n",
    "    plt.savefig(\n",
    "        \"eticaret\" + metric + \".png\",\n",
    "        dpi=(250),\n",
    "        bbox_inches=\"tight\",\n",
    "        transparent=transparent,\n",
    "    )\n",
    "\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "yorumlar = pd.read_csv(\"yorumlar.csv\")\n",
    "target = yorumlar[\"Puan\"].values.tolist()\n",
    "data = yorumlar[\"Yorum\"].values.tolist()\n",
    "\n",
    "cutoff = int(len(data) * 0.90)\n",
    "x_train, x_test = data[:cutoff], data[cutoff:]\n",
    "y_train, y_test = target[:cutoff], target[cutoff:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tf_tokenizer.json') as f:\n",
    "    json_string = json.load(f)\n",
    "\n",
    "tf_tokenizer = tf.keras.preprocessing.text.tokenizer_from_json(json_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenlestir(yorumListesi):\n",
    "    y_yorumlar = []\n",
    "    for yorum in yorumListesi:\n",
    "        y_yorum = tf_tokenizer.texts_to_sequences(str(yorum).lower())[0][:50]\n",
    "\n",
    "        if len(y_yorum) < 50:\n",
    "            sifirlar = list(np.zeros(50 - len(y_yorum), dtype=int))\n",
    "            y_yorum = sifirlar + y_yorum\n",
    "\n",
    "        y_yorumlar.append(y_yorum)\n",
    "    return np.array(y_yorumlar, dtype=np.dtype(np.int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "egitim_kume = tokenlestir(x_train)\n",
    "test_kume = tokenlestir(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "196544"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tf_tokenizer.word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_layer (Embedding)  (None, 50, 50)            9827200   \n",
      "_________________________________________________________________\n",
      "gru_3 (GRU)                  (None, 50, 16)            3216      \n",
      "_________________________________________________________________\n",
      "gru_4 (GRU)                  (None, 50, 8)             600       \n",
      "_________________________________________________________________\n",
      "gru_5 (GRU)                  (None, 4)                 156       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 9,831,177\n",
      "Trainable params: 9,831,177\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(\n",
    "    Embedding(input_dim=len(tf_tokenizer.word_counts), output_dim=50, input_length=50, name=\"embedding_layer\")\n",
    ")\n",
    "\n",
    "model.add(GRU(units=16, return_sequences=True, reset_after=False))\n",
    "model.add(GRU(units=8, return_sequences=True, reset_after=False))\n",
    "model.add(GRU(units=4, return_sequences=False, reset_after=False))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "opt = adam_v2.Adam(learning_rate=1e-3, clipnorm=1.0, clipvalue=0.5)\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_y_train = np.array(y_train)\n",
    "n_y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.6689 - accuracy: 0.5994WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 30 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 30 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140/140 [==============================] - 31s 179ms/step - loss: 0.6689 - accuracy: 0.5994 - val_loss: 0.6530 - val_accuracy: 0.6272\n",
      "Epoch 2/25\n",
      "140/140 [==============================] - 24s 171ms/step - loss: 0.6473 - accuracy: 0.6342\n",
      "Epoch 3/25\n",
      "140/140 [==============================] - 24s 170ms/step - loss: 0.6468 - accuracy: 0.6347\n",
      "Epoch 4/25\n",
      "140/140 [==============================] - 24s 169ms/step - loss: 0.6468 - accuracy: 0.6343\n",
      "Epoch 5/25\n",
      "140/140 [==============================] - 24s 169ms/step - loss: 0.6467 - accuracy: 0.6346\n",
      "Epoch 6/25\n",
      "140/140 [==============================] - 23s 167ms/step - loss: 0.6467 - accuracy: 0.6341\n",
      "Epoch 7/25\n",
      "140/140 [==============================] - 23s 166ms/step - loss: 0.6465 - accuracy: 0.6346\n",
      "Epoch 8/25\n",
      "140/140 [==============================] - 23s 167ms/step - loss: 0.6466 - accuracy: 0.6346\n",
      "Epoch 9/25\n",
      "140/140 [==============================] - 23s 166ms/step - loss: 0.6466 - accuracy: 0.6347\n",
      "Epoch 10/25\n",
      "140/140 [==============================] - 24s 168ms/step - loss: 0.6466 - accuracy: 0.6343\n",
      "Epoch 11/25\n",
      "140/140 [==============================] - 23s 166ms/step - loss: 0.6466 - accuracy: 0.6344\n",
      "Epoch 12/25\n",
      "140/140 [==============================] - 23s 167ms/step - loss: 0.6466 - accuracy: 0.6342\n",
      "Epoch 13/25\n",
      "140/140 [==============================] - 24s 167ms/step - loss: 0.6465 - accuracy: 0.6347\n",
      "Epoch 14/25\n",
      "140/140 [==============================] - 23s 166ms/step - loss: 0.6466 - accuracy: 0.6347\n",
      "Epoch 15/25\n",
      "140/140 [==============================] - 23s 167ms/step - loss: 0.6465 - accuracy: 0.6346\n",
      "Epoch 16/25\n",
      "140/140 [==============================] - 24s 168ms/step - loss: 0.6465 - accuracy: 0.6347\n",
      "Epoch 17/25\n",
      "140/140 [==============================] - 24s 173ms/step - loss: 0.6465 - accuracy: 0.6345\n",
      "Epoch 18/25\n",
      "140/140 [==============================] - 24s 171ms/step - loss: 0.6465 - accuracy: 0.6344\n",
      "Epoch 19/25\n",
      "140/140 [==============================] - 24s 171ms/step - loss: 0.6466 - accuracy: 0.6348\n",
      "Epoch 20/25\n",
      "140/140 [==============================] - 24s 171ms/step - loss: 0.6464 - accuracy: 0.6347\n",
      "Epoch 21/25\n",
      "140/140 [==============================] - 24s 171ms/step - loss: 0.6465 - accuracy: 0.6347\n",
      "Epoch 22/25\n",
      "140/140 [==============================] - 24s 172ms/step - loss: 0.6466 - accuracy: 0.6347\n",
      "Epoch 23/25\n",
      "140/140 [==============================] - 24s 171ms/step - loss: 0.6465 - accuracy: 0.6347\n",
      "Epoch 24/25\n",
      "140/140 [==============================] - 24s 169ms/step - loss: 0.6464 - accuracy: 0.6346\n",
      "Epoch 25/25\n",
      "140/140 [==============================] - 24s 170ms/step - loss: 0.6466 - accuracy: 0.6347\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    egitim_kume,\n",
    "    n_y_train,\n",
    "    epochs=25,\n",
    "    batch_size=512,\n",
    "    validation_data=(test_kume, n_y_test),\n",
    "    validation_steps=30,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "249/249 [==============================] - 3s 12ms/step - loss: 0.4711 - accuracy: 0.8697\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_kume, n_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.47108957171440125\n",
      "Test Accuracy: 0.8696581125259399\n",
      "[[0.9590216 ]\n",
      " [0.95821506]\n",
      " [0.01047284]\n",
      " [0.95859253]\n",
      " [0.01047935]\n",
      " [0.220978  ]\n",
      " [0.95817643]\n",
      " [0.01048765]\n",
      " [0.9576007 ]\n",
      " [0.18795137]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Loss: {}\".format(test_loss))\n",
    "print(\"Test Accuracy: {}\".format(test_acc))\n",
    "\n",
    "text1 = \"bu ürün çok iyi herkese tavsiye ederim\"\n",
    "text2 = \"kargo çok hızlı aynı gün elime geçti\"\n",
    "text3 = \"büyük bir hayal kırıklığı yaşadım bu ürün bu markaya yakışmamış\"\n",
    "text4 = \"mükemmel\"\n",
    "text5 = \"tasarımı harika ancak kargo çok geç geldi ve ürün açılmıştı tavsiye etmem\"\n",
    "text6 = \"hiç resimde gösterildiği gibi değil\"\n",
    "text7 = \"kötü yorumlar gözümü korkutmuştu ancak hiçbir sorun yaşamadım teşekkürler\"\n",
    "text8 = \"hiç bu kadar kötü bir satıcıya denk gelmemiştim ürünü geri iade ediyorum\"\n",
    "text9 = \"tam bir fiyat performans ürünü\"\n",
    "text10 = \"beklediğim gibi çıkmadı\"\n",
    "yazilar = [text1, text2, text3, text4, text5, text6, text7, text8, text9, text10]\n",
    "\n",
    "deneme_kume = tokenlestir(yazilar)\n",
    "sonuc = model.predict(deneme_kume)\n",
    "print(sonuc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graph_accuracy(history, \"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graph_loss(history, \"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/my_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/my_model/assets\n"
     ]
    }
   ],
   "source": [
    "# Save the entire model as a SavedModel.\n",
    "!mkdir -p saved_model\n",
    "model.save('tf_saved_model/my_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_layer (Embedding)  (None, 50, 50)           1000050   \n",
      "                                                                 \n",
      " gru_12 (GRU)                (None, 50, 16)            3216      \n",
      "                                                                 \n",
      " gru_13 (GRU)                (None, 50, 8)             600       \n",
      "                                                                 \n",
      " gru_14 (GRU)                (None, 4)                 156       \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 5         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,004,027\n",
      "Trainable params: 1,004,027\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "new_model = tf.keras.models.load_model('tf_saved_model/my_model')\n",
    "\n",
    "# Check its architecture\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "249/249 [==============================] - 3s 12ms/step\n",
      "[[1609 2402]\n",
      " [1941 2004]]\n"
     ]
    }
   ],
   "source": [
    "# confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "y_pred = new_model.predict(test_kume)\n",
    "y_pred = (y_pred > 0.5)\n",
    "\n",
    "cm = confusion_matrix(n_y_test, y_pred)\n",
    "print(cm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
